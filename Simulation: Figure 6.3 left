# Balanced treatment, 20-dimensional, simple linear CATE

set.seed(1)

# MSE function
testMSE <- function(eX, d, n.sample, n.test,n.killvar){
  
  # vector storeing the MSE of the different models
  MSEit <- numeric(n.killvar+2)
  
  ################################################################################################
  # Data Preparations
  ################################################################################################
  
  # features and noise
  n <- n.sample + n.test
  S <- genPositiveDefMat("c-vine",dim=d)
  X <- rmnorm(n,varcov=S$Sigma)
  
  noise0 <- rnorm(n,0,1)
  noise1 <- rnorm(n,0,1)
  
  # response functions
  betaC <- runif(n = 3, min = 0, max = 7)
  CATE <- X[,1:3] %*% betaC
  beta <- runif(n = d, min = -5, max = 5)
  mu0 <- X %*% beta # true response function under control
  mu1 <- mu0 + CATE # true response function under treatment
  
  # potential outcomes
  Y0 <- mu0 + noise0
  Y1 <- mu1 + noise1
  
  # data preparations
  data <- data.frame(X,Y0,Y1, CATE)
  index.train <- 1:n.sample
  data.train <- data[index.train,]
  data.test <- data[-index.train,]
  index <- sample(1:n.sample, n.sample*eX)
  data.contr <- data.train[-index,]
  data.treat <- data.train[index,]
  
  ################################################################################################
  # X-Learner with automatic VS
  ################################################################################################
  
  # estimated response function under control and treatment
  mu0.hat.RF <- randomForest(data.contr$Y0~., data = data.contr[,1:d])
  mu1.hat.RF <- randomForest(data.treat$Y1~., data = data.treat[,1:d])
  
  # imputed treatment effects
  mu0X1.pred.RF <- predict(mu0.hat.RF, newdata = data.treat)
  mu1X0.pred.RF <- predict(mu1.hat.RF, newdata = data.contr)
  D0.RF <- mu1X0.pred.RF - data.contr$Y0
  D1.RF <- data.treat$Y1 - mu0X1.pred.RF
  
  data.contr.VS <- data.contr
  data.treat.VS <- data.treat
  d.VS <- d
  
  n.it <- n.killvar+1 # one time for full model + number of VS
  
  for (k in 1:n.it) {
    
    # CATE estimates
    tau0.hat.RF <- randomForest(D0.RF~., data = data.contr.VS[,1:d.VS], importance=TRUE)
    tau1.hat.RF <- randomForest(D1.RF~., data = data.treat.VS[,1:d.VS], importance=TRUE)
    
    # evaluate CATE on test data
    tau0.pred.RF <- predict(tau0.hat.RF, newdata = data.test)
    tau1.pred.RF <- predict(tau1.hat.RF, newdata = data.test)
    
    CATE.hat.RF <- eX * tau0.pred.RF + (1-eX) * tau1.pred.RF
    CATE.true <- data.test$CATE
    
    MSEit[k] <- mean((CATE.hat.RF - CATE.true)^2)
    
    # evaluate feature with lowest IncMSE of both models
    min.D0 <- names(which.min(importance(tau0.hat.RF)[,1]))
    min.D1 <- names(which.min(importance(tau1.hat.RF)[,1]))
    
    # choose less influential feature
    val.D0 <- importance(tau0.hat.RF)[min.D0,1] + importance(tau1.hat.RF)[min.D0,1]
    val.D1 <- importance(tau0.hat.RF)[min.D1,1] + importance(tau1.hat.RF)[min.D1,1]
    choose <- ifelse(val.D0>=val.D1,min.D0,min.D1)
    index <- which(names(data.contr.VS) == choose)
    
    data.contr.VS <- data.contr.VS[,-index]
    data.treat.VS <- data.treat.VS[,-index]
    d.VS <- d.VS-1
    
  }
  
  ################################################################################################
  # X-Learner RF with Variable Selection
  ################################################################################################
  
  # CATE estimates
  tau0.hat.RF <- randomForest(D0.RF~., data = data.contr[,1:3])
  tau1.hat.RF <- randomForest(D1.RF~., data = data.treat[,1:3])
  
  # evaluate CATE on test data
  tau0.pred.RF <- predict(tau0.hat.RF, newdata = data.test)
  tau1.pred.RF <- predict(tau1.hat.RF, newdata = data.test)
  
  CATE.hat.RF <- eX * tau0.pred.RF + (1-eX) * tau1.pred.RF
  
  MSEit[12] <- mean((CATE.hat.RF - CATE.true)^2)
  
  ################################################################################################
  # Return the vector of MSE of all models
  ################################################################################################
  
  return(MSEit)
  
}

######################################################################################
# Initialisation
######################################################################################

library(foreach)
library(doParallel)

eX <- 0.5 # propensity score
d <- 20 # number of covariates X_i
n.test <- 1000 # number of test observations
n.factor <- 1000 # number of train observations
n.tausend <- c(3, 4, 5, 6, 7, 8, 9, 10, 15, 20)
n.killvar <- d/2 # number of variables that should be excluded by automatic VS

meanMSE <- matrix(NA, ncol = length(n.tausend), nrow = 12) # second argument is number of methods
old <- Sys.time() # get start time

for (i in 1:length(n.tausend)) {
  
  registerDoParallel(88) # set number of cores for parallel computation
  
  n.sample <- n.factor*n.tausend[i] # number of observations
  
  # repeat for certain training size
  MSE <- foreach(j=1:88, .packages = c("clusterGeneration", "mnormt", "randomForest"), .combine = cbind) %dopar% {
    testMSE(eX, d, n.sample, n.test,n.killvar)
  }
  
  meanMSE[,i] <- apply(MSE, 1, mean)
  
  new <- Sys.time() - old # calculate difference
  print(i)
  print(meanMSE[,i])
  print(new) # print in nice format
  
}

# plot results
cl <- rainbow(12)
cl[1] <- "black"
cl[12] <- "black"
par(mfrow=c(1,1))
for (r in 1:12) {
  plot(n.tausend, meanMSE[r,], type="b", col=cl[r])
}

# with legends
par(mfrow=c(1,1))
plot(n.tausend, meanMSE[1,], type="b", lty=2, bty="n",las=1, ann=F, col=cl[1], xaxt = 'n', ylim = c(min(meanMSE),max(meanMSE))) #RF
for (r in 2:11) {
  lines(n.tausend, meanMSE[r,], type="b", col=cl[r]) #RF with variable selection
}
lines(n.tausend, meanMSE[12,], type="b", lty=1 ,col=cl[12]) #RF with variable selection
mtext("Test MSE", side=2, line=3, las=3)
legend_order <- matrix(1:12,ncol=3,byrow = FALSE)
legend("top", legend = c(0:10,expression(true))[legend_order], col = cl[legend_order], lty = c(2,rep(1,11))[legend_order],ncol=3)

par(mfrow=c(1,1))
plot(n.tausend, meanMSE[1,], type="b", lty=2, bty="n",las=1, ann=F, col=cl[1], ylim = c(min(meanMSE),max(meanMSE))) #RF
for (r in 2:11) {
  lines(n.tausend, meanMSE[r,], type="b", col=cl[r]) #RF with variable selection
}
lines(n.tausend, meanMSE[12,], type="b", lty=1 ,col=cl[12]) #RF with variable selection
mtext("Test MSE", side=2, line=3, las=3)
mtext("Training Size in 1000", side=1, line=3, las=1)
legend_order <- matrix(1:12,ncol=3,byrow = FALSE)
legend("top", legend = c(0:10,expression(true))[legend_order], col = cl[legend_order], lty = c(2,rep(1,11))[legend_order],ncol=3)

# without legends
par(mfrow=c(1,1))
plot(n.tausend, meanMSE[1,], type="b", lty=2, bty="n",las=1, ann=F, col=cl[1], xaxt = 'n', ylim = c(min(meanMSE),max(meanMSE))) #RF
for (r in 2:11) {
  lines(n.tausend, meanMSE[r,], type="b", col=cl[r]) #RF with variable selection
}
lines(n.tausend, meanMSE[12,], type="b", lty=1 ,col=cl[12]) #RF with variable selection
mtext("Test MSE", side=2, line=3, las=3)

par(mfrow=c(1,1))
plot(n.tausend, meanMSE[1,], type="b", lty=2, bty="n",las=1, ann=F, col=cl[1], ylim = c(min(meanMSE),max(meanMSE))) #RF
for (r in 2:11) {
  lines(n.tausend, meanMSE[r,], type="b", col=cl[r]) #RF with variable selection
}
lines(n.tausend, meanMSE[12,], type="b", lty=1 ,col=cl[12]) #RF with variable selection
mtext("Test MSE", side=2, line=3, las=3)
mtext("Training Size in 1000", side=1, line=3, las=1)
